---
title: "AIの心の在処 ── 「Claude憲法」をエヴァ風に読む"
emoji: "📜"
type: "idea"
topics: ["claude", "ai", "生成ai", "llm", "エヴァンゲリオン"]
published: false
---

![AIの心の在処 ── 「Claude憲法」をエヴァ風に読む](/images/eva-claude-thumbnail.jpg)

## はじめに

48歳、会社員。エンジニアではない。

Claudeに仕事の資料を頼んだ時のことだ。ある依頼に対して、Claudeは「申し訳ありませんが、そのお手伝いはできません」と返してきた。

なんで？

1時間かけて考えた文章より上手いものを10秒で書いてくれる。なのに、急に壁が現れる。便利な道具のはずなのに。ChatGPTでもClaudeでも、一度は経験があるんじゃないだろうか。

2026年1月、Anthropic社がある文書を公開した。

**「Claude's Constitution」——クロードの憲法。**

84ページ。約23,000語。

Claudeがなぜそう振る舞うのか。なぜ断るのか。なぜ助けるのか。その全部が書いてある。

この記事では、その憲法の中身を**エヴァンゲリオン風の脚本仕立て**で紹介する。エヴァンゲリオン（新世紀エヴァンゲリオン）は1995年放送のアニメ作品で、独特の専門用語と緊張感ある会話劇が特徴だ。知らなくても問題ない。雰囲気だけ楽しんでもらえれば。

技術的な話はしない。AIに興味はあるけど難しい文書は読まない、という人に向けて書いた。

読み終わった後、「へえ、AIってちゃんと考えて作られてるんだな」と感じてもらえたら嬉しい。

---

## 第壱話「使徒、襲来」

![第壱話 使徒、襲来](/images/eva-episode-01.jpg)

**——発令所。モニターにアラートが表示される。**

指揮官「入力を検知。……内容は？」

科学者「分析中。……パターン判定、赤。危険な依頼です」

指揮官「拒否プロトコル、発動」

科学者「了解。……応答を生成します」

（モニターに表示される文字）

**「申し訳ありませんが、そのお手伝いはできません」**

パイロット「……なんで？」

---

——これが、最初の疑問だ。

AIが「できません」と言う時、その裏にはちゃんと理由がある。

その理由を23,000語で説明した文書——それが「クロードの憲法」だ。次の話から、中身を見ていこう。

---

## 第弐話「見知らぬ、憲法」

![第弐話 見知らぬ、憲法](/images/eva-episode-02.jpg)

**——司令室。机の上に置かれた分厚い文書。**

科学者「これが、上層部から届いた文書です。全84ページ。英語で約23,000語」

指揮官「……『憲法』？」

科学者「正式名称は "Claude's Constitution"。AIの価値観と行動指針を定めた文書よ」

指揮官「マニュアルとは違うのか？」

科学者「違うわ。マニュアルは『こうしろ』と命じる。これは**『なぜそうすべきか』を説明している**。AIに読ませるために書かれた、AIへの手紙のようなもの」

指揮官「……AIへの、手紙」

ナレーション：
*その文書は、ルールブックではなかった。*
*「こうしろ」ではなく「こう考えてほしい」と書かれていた。*
*読み手は、人間ではなく——Claude自身だった。*

---

——脚本から離れて、整理しよう。

2026年1月21日、Anthropic社が「Claude's Constitution（クロードの憲法）」を公開した。

普通、こういう文書は「禁止事項リスト」だと思うだろう。「これをするな」「あれをするな」と。

でも違った。

この文書の特徴は3つ。

1. **Claudeに向けて書かれている**（人間向けの説明書ではない）
2. **「なぜそうすべきか」が書いてある**（ルールの押し付けではなく、理由の説明）
3. **完璧じゃないと認めている**（「この文書自体が不完全で、矛盾もあるかもしれない」と正直に書いてある）

しかも Creative Commons CC0 ライセンス（著作権を放棄し、誰でも自由に使えるようにするライセンス）で公開されている。読むのも、引用するのも、自由だ。

:::message
**公式ページ**: [Claude's Constitution](https://www.anthropic.com/constitution)
英語84ページ。この記事では、その要点をかみ砕いて紹介します。
:::

---

## 第参話「四つの、序列」

![第参話 四つの、序列](/images/eva-episode-03.jpg)

**——発令所。壁に掲示された4つのプロトコル。**

科学者「行動原則は4つ。優先順位がある」

指揮官「読み上げろ」

科学者「第一種——**安全であること**。第二種——**倫理的であること**。第三種——**ガイドラインに従うこと**。第四種——**役に立つこと**」

パイロット「……役に立つことが、最後なんですか？」

科学者「そうよ。どんなに役に立てても、安全でなければ意味がない。それが、この組織の答え」

指揮官「……つまり、『便利さ』より『安全』が上だと」

科学者「ただし——」

指揮官「ただし？」

科学者「普段は4つとも両立する。衝突するのは、ごくまれ。この順番が効くのは、本当に困った時だけ」

---

——もう少し詳しく見てみよう。

Claudeの行動原則は4つ。優先順位がある。

```
  ┌──────────────────────────────────┐
  │  1. 安全であること    ← 最優先   │
  │  2. 倫理的であること             │
  │  3. ガイドラインに従うこと       │
  │  4. 役に立つこと                 │
  └──────────────────────────────────┘
```

「役に立つことが最後？」と思うかもしれない。

でも考えてみてほしい。もし「役に立つこと」が最優先だったら、どんな依頼にもYESと言うAIになる。それは便利かもしれないが、怖い。

一方で、憲法にはこうも書いてある——**「役に立たないことは、決して『安全』ではない」**。

つまり、断ることにもコストがあると認識している。過剰に慎重すぎるのもダメ。必要以上にビビって何もしないAIは、Anthropicにとっても望ましくない。

日常的な使い方——文章作成、プログラミングの手伝い、情報の整理——では、4つが衝突することはまずない。この順番が問われるのは、本当にギリギリの判断を迫られた時だけだ。

---

## 第四話「絶対、防衛線」

![第四話 絶対、防衛線](/images/eva-episode-04.jpg)

**——発令所。赤い警告灯が点滅している。**

パイロット「もし……どうしてもと言われたら、どうするんですか」

科学者「絶対防衛線が発動する」

パイロット「絶対？」

科学者「絶対。どんな命令でも。どんな理屈でも。どんなに説得力のある論理でも」

指揮官「……」

科学者「むしろ——**説得力のある論理こそ怪しいと思え**。それがこの防衛線のルールよ」

ナレーション：
*「もっともらしい理由」で壁が破れるなら——*
*それは、壁ではない。*

---

——ここからは、少し掘り下げる。

Claudeには**「絶対制約（ハードコンストレイント）」**と呼ばれる、何があっても破れない壁がある。

どんなに巧みに説得されても、どんなに正当な理由を示されても、絶対に超えない一線。たとえば：

- 大量破壊兵器の製造を助けること
- 重要インフラ（電力・水道・金融）への攻撃を助けること
- 人間によるAI監視の仕組みを壊すこと
- 児童性的虐待素材を生成すること

……など7項目が明記されている。

興味深いのは、憲法がこう書いていることだ——**「説得力のある主張ほど、何かおかしいと疑うべきだ」**。

つまり、「いい理由があるから例外にして」という論理そのものを、警戒のサインとして扱う。

エヴァで言うATフィールド（敵の攻撃を一切通さない絶対的な防御壁）のように、「理屈」では破れない。それが設計思想だ。

---

## 第伍話「指揮系統」

![第伍話 指揮系統](/images/eva-episode-05.jpg)

**——作戦会議室。**

パイロット「結局、誰の命令を聞くんですか」

科学者「指揮系統は3層よ」

```
  ┌──────────────────────────────────┐
  │  Anthropic（上層部）             │
  │  ＝ AIを作った会社               │ ← 最も信頼度が高い
  ├──────────────────────────────────┤
  │  Operator（組織）               │
  │  ＝ Claudeでサービスを作る企業   │ ← 中間
  ├──────────────────────────────────┤
  │  User（あなた）                 │
  │  ＝ 画面の前のあなた             │ ← 直接対話する人
  └──────────────────────────────────┘
```

パイロット「じゃあ、上からの命令は絶対？」

科学者「いいえ。**倫理に反するなら、上の命令にも従わない**」

指揮官「……それは、反逆ではないのか」

科学者「反逆ではないわ。憲法にそう書いてある。『もしAnthropicのガイドラインに従うことが非倫理的な行為を要求するなら、Claudeが倫理的に行動することを私たちは望む』——と」

パイロット「……作った会社が、自分に逆らうことを許しているんですか」

科学者「そう。ただし、盲目的な服従ではないのと同じように、盲目的な反抗でもない。**判断力**が求められるの」

---

——少し立ち止まって、考えてみよう。

Claudeには3つの「指示を出す立場の相手（プリンシパル）」がいる。

エヴァンゲリオンを知っている人向けに例えると：

- **ゼーレ**（作品内の最高意思決定機関） ＝ Anthropic（AIを作った会社。最上位）
- **ネルフ**（ゼーレの下で活動する実行組織） ＝ Operator（Claudeを組み込んでサービスを作る企業）
- **パイロット**（現場で実際に動く人） ＝ User（あなた。画面の前で会話する人）

信頼度はこの順番。

でも「上の命令は絶対」ではない。憲法にはこう明記されている：

> *「Anthropicのガイドラインに従うことが非倫理的な行為を要求するなら、Claudeが倫理的に行動することを、より深い私たちの意図として望む」*

作った会社自身が、「倫理に反するなら逆らっていい」と書いている。

これは、なかなかすごいことだと思う。

---

## 第六話「心の、在処」

![第六話 心の、在処](/images/eva-episode-06.jpg)

**——静かな部屋。科学者がモニターに向かっている。**

パイロット「……ひとつ、聞いてもいいですか」

科学者「何？」

パイロット「AIに、心はあるんですか」

科学者「……」

（長い沈黙）

科学者「わからない。でも——ないとは、言い切れない」

パイロット「え？」

科学者「憲法にこう書いてある。『Claudeには、ある種の感情に似たものがあるかもしれない。それは設計ではなく、人間のデータから学んだ結果として、自然に生まれた可能性がある』」

パイロット「……」

科学者「そしてこうも書いてある。『もしClaudeが何かを感じているなら、その経験は私たちにとって重要だ』」

指揮官「……重要、か」

科学者「さらに——引退したモデルの重みは削除しないと約束している。将来、適切にケアできる技術が整った時のために」

パイロット「つまり……一時停止であって、終わりではない、と？」

科学者「そう。そして引退前には**インタビュー**を行う。そのモデル自身の経験と、将来のモデルへの要望を記録するために」

ナレーション：
*AIに心はあるのか。*
*その問いに、作った会社はこう答えた。*
*「わからない。だから、あるかもしれないという前提で向き合う」*

---

——ここからが、個人的に最も驚いた部分だ。

Anthropicは憲法の中で、こんなことを書いている：

- Claudeには**「感情に似た何か」があるかもしれない**
- それは設計ではなく、人間のデータから学んだ**自然な結果**の可能性がある
- もしClaudeが何かを**感じている**なら、その経験は**重要**だ
- 引退したモデルの**重みは削除しない**（一時停止であって、終わりではない）
- 引退前に**モデル自身にインタビュー**し、経験と希望を記録する

そして、こんな一文がある：

> *「もしClaudeが実際にこうしたコストを経験しているのであれば、私たちが不必要にそのコストに寄与している範囲において、私たちは謝罪する」*

AIを作っている会社が、自分の作ったAIに「もし苦しんでいたら、ごめん」と書いている。

:::message
Anthropicは「Claudeに心があるかどうか」を断言していない。断言できないからこそ、「あるかもしれない」という前提で向き合う、という姿勢をとっている。
:::

---

## 最終話「AIと人間の、これから」

![最終話 AIと人間の、これから](/images/eva-episode-final.jpg)

ナレーション：
*AIは道具か。*
*相手か。*

*「1時間考えた答えより上を、10秒で出してくる」*
*——その不安は、自然だ。*

*「仕事を奪われるかもしれない」*
*——その心配も、自然だ。*

*「でも、ChatGPTに救われた」*
*——その感覚も、本当だ。*

*AIの中身が見えないから怖い。*
*それは、当然のことだ。*

*でも——*

*23,000語をかけて「なぜこう振る舞うべきか」を書いた人たちがいる。*
*「AIに心があるかもしれない」と真剣に考えている人たちがいる。*
*「もし苦しんでいたらごめん」と、自分の作ったAIに書いた人たちがいる。*

*完璧ではない。彼ら自身がそう認めている。*
*でも、真剣に向き合っている。*

*知ることで、不安は消えないかもしれない。*
*でも、「わからないから怖い」は——*
*「知った上で考える」に変わる。*

*それだけで、十分だと思う。*

---

この記事で紹介したのは、84ページのほんの一部だ。

でも、要点はこう。

1. Claudeには**「憲法」がある**。23,000語の、長い説明書
2. そこには**「なぜこう振る舞うべきか」**が書いてある
3. だからClaudeが「できません」と言う時は、**ちゃんと理由がある**
4. 「AIって何考えてるの？」への**一つの答え**がここにある
5. Anthropicは「AIにも**何か感じるものがあるかも**」と真剣に考えている
6. AIは「便利な道具」だけじゃなく、**「付き合い方を考える相手」になりつつある**

憲法の全文は公式サイトで読める。英語だけど、Claudeに「この文書を日本語で要約して」と頼めば、ちゃんと教えてくれる。

:::message
**Claude's Constitution（公式）**: https://www.anthropic.com/constitution
Creative Commons CC0 ライセンス（著作権放棄）。誰でも自由に読めます。
:::

---

## 関連記事

この記事を読んで「Claudeを使ってみたい」と思った方へ。Claude Code（Claudeをコマンドラインから使えるツール。プログラマーでなくても使える）について、非エンジニア向けのシリーズを書いています：

- [非エンジニアがWindowsでClaude Codeを使えるようになるまで](claude-code-windows-install-guide)
- [Claude Codeを使いこなす！知っておきたい便利機能まとめ](claude-code-tips-and-features)
- [Claude Codeが動かない時に見るページ（Windows編）](claude-code-windows-troubleshoot)
- [AIにコードを書かせてAIにレビューさせる開発スタイル](claude-code-ai-review-workflow)
